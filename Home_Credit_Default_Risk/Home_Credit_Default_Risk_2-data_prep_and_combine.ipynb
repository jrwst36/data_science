{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home Credit Default Risk: Data Prep and Combine\n",
    "In this notebook the data will be prepared and combined for subsequent machine learning. These steps will be as follows:\n",
    "- Primary data (train and test data sets)\n",
    "    - Address outliers in the train and test data sets\n",
    "    - Imput null values in the train and test data sets\n",
    "    - Get dummy variables on the object data\n",
    "- bureau and bureau_balance data sets\n",
    "    - Address outliers in bureau\n",
    "    - In bureau_balance group by SK_ID_BUREAU and sum\n",
    "    - Get dummy variables on the object data for both data sets\n",
    "    - Join bureau_balance to bureau on SK_ID_BUREAU\n",
    "- previous_application and dependents\n",
    "    - Address outliers in the previous_application data set these are all in the DAYS... columns.\n",
    "    - Get dummy variables on the object data.\n",
    "    - Consolidate the data grouping by SK_ID_CURR and aggregating by sum for object features, and mean for numeric features.\n",
    "- Join all data sets on SK_ID_CURR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Load the data\n",
    "train = pd.read_csv('data/application_train.csv', index_col='SK_ID_CURR')\n",
    "test = pd.read_csv('data/application_test.csv', index_col='SK_ID_CURR')\n",
    "bureau = pd.read_csv('data/bureau.csv')\n",
    "bureau_balance = pd.read_csv('data/bureau_balance.csv')\n",
    "prev_app = pd.read_csv('data/previous_application.csv')\n",
    "cc_balance = pd.read_csv('data/credit_card_balance.csv')\n",
    "payments = pd.read_csv('data/installments_payments.csv')\n",
    "cash_balance = pd.read_csv('data/POS_CASH_balance.csv')\n",
    "\n",
    "target = train.TARGET\n",
    "train = train.drop('TARGET', axis='columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Preparation of the primary data\n",
    "## Address outliers\n",
    "DAYS_EMPLOYED has values at 365243 which is 1000 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the medain excluding these outliers\n",
    "day_employed_median = train.loc[train.DAYS_EMPLOYED != 365243, 'DAYS_EMPLOYED'].median()\n",
    "# Apply this value to the outliers in the train and test data sets\n",
    "train.loc[train.DAYS_EMPLOYED == 365243, 'DAYS_EMPLOYED'] = day_employed_median\n",
    "test.loc[test.DAYS_EMPLOYED == 365243, 'DAYS_EMPLOYED'] = day_employed_median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REGION_RATING_CLIENT_W_CITY has a negative value (-1). This is an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_rating_client_w_city_median = train.REGION_RATING_CLIENT_W_CITY.median()\n",
    "test.loc[test.REGION_RATING_CLIENT_W_CITY == -1, 'REGION_RATING_CLIENT_W_CITY'] = region_rating_client_w_city_median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few instances of categorical data in the training set but not in the test set. I will simply remove these observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307500, 120)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the few instances of categorical data in the training set but not in the test set\n",
    "train.drop(train[train.CODE_GENDER == 'XNA'].index, axis='rows',inplace=True)\n",
    "train.drop(train[train.NAME_INCOME_TYPE == 'Maternity leave'].index, axis='rows',inplace=True)\n",
    "train.drop(train[train.NAME_FAMILY_STATUS == 'Unknown'].index, axis='rows', inplace=True)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute null values\n",
    "Null values in object columns will be imputed with the string 'no_value'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_columns = train.select_dtypes(include='object').columns\n",
    "\n",
    "train[obj_columns] = train[obj_columns].fillna('no_value')\n",
    "test[obj_columns] = test[obj_columns].fillna('no_value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null values in the numberic columns will be filled with the median value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_columns = train.select_dtypes(exclude='object').columns\n",
    "\n",
    "for column in num_columns:\n",
    "    median_value = train[column].median()\n",
    "    train[column].fillna(median_value, inplace=True)\n",
    "    test[column].fillna(median_value, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.get_dummies(train)\n",
    "test = pd.get_dummies(test)\n",
    "\n",
    "# Address any missing columns in each set\n",
    "train_col_set = set(train.columns)\n",
    "test_col_set = set(test.columns)\n",
    "train_missing = list(test_col_set.difference(train_col_set))\n",
    "test_missing = list(train_col_set.difference(test_col_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only test has missing columns. The columns will be added to the test set and 0 will be broadcast to the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in test_missing:\n",
    "    test[column] = 0\n",
    "    \n",
    "# Make sure the column order is the same for train and test\n",
    "train_columns = train.columns\n",
    "test = test[train_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Preparation of bureau and bureau_balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Address outliers\n",
    "DAYS_ENDDATED_FACT in bureau has an outlier as -42023. This will be set to the median value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_enddate_fact_medain = bureau.loc[bureau.DAYS_ENDDATE_FACT != -42023, 'DAYS_ENDDATE_FACT'].median()\n",
    "bureau.loc[bureau.DAYS_ENDDATE_FACT == -42023, 'DAYS_ENDDATE_FACT'] = days_enddate_fact_medain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare bureau_balance to combine with bureau\n",
    "First get dummy variables, then group by SK_ID_BUREAU, finally sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_balance = pd.get_dummies(bureau_balance)\n",
    "bureau_balance = bureau_balance.groupby(['SK_ID_BUREAU']).sum()\n",
    "\n",
    "# Remove the MONTHS_BALANCE columns\n",
    "bureau_balance.drop('MONTHS_BALANCE', axis='columns',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join bureau and bureau_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau = bureau.join(bureau_balance, on='SK_ID_BUREAU')\n",
    "\n",
    "# Remove the 'SK_ID_BUREAU' columns\n",
    "bureau.drop('SK_ID_BUREAU', axis='columns', inplace=True)\n",
    "\n",
    "# Get dummies on oject columns\n",
    "bureau = pd.get_dummies(bureau)\n",
    "\n",
    "# Group by SK_ID_CURR and sum each column\n",
    "bureau = bureau.groupby(['SK_ID_CURR']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Preparation of previous_application and dependents\n",
    "First drop unneeded columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_app.drop('SK_ID_PREV', axis='columns', inplace=True)\n",
    "cc_balance.drop('SK_ID_PREV', axis='columns', inplace=True)\n",
    "payments.drop('SK_ID_PREV', axis='columns', inplace=True)\n",
    "cash_balance.drop('SK_ID_PREV', axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the \"SK_ID_CURR\" column as type int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_app['SK_ID_CURR'] = prev_app.SK_ID_CURR.astype('int')\n",
    "cc_balance['SK_ID_CURR'] = cc_balance.SK_ID_CURR.astype('int')\n",
    "payments['SK_ID_CURR'] = payments.SK_ID_CURR.astype('int')\n",
    "cash_balance['SK_ID_CURR'] = cash_balance.SK_ID_CURR.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Address outliers\n",
    "In the previous_application data set there are many entries listed at 365243. This will be set the median of that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SK_ID_CURR\n",
      "DAYS_FIRST_DRAWING\n",
      "DAYS_FIRST_DUE\n",
      "DAYS_LAST_DUE_1ST_VERSION\n",
      "DAYS_LAST_DUE\n",
      "DAYS_TERMINATION\n"
     ]
    }
   ],
   "source": [
    "# Columns with the outliers\n",
    "outlier_columns = []\n",
    "for column in prev_app.columns:\n",
    "    if (prev_app[column].values == 365243).any():\n",
    "        print(column)\n",
    "        outlier_columns.append(column)\n",
    "\n",
    "# Remove the SK_ID_CURR column. That is not an outlier\n",
    "outlier_columns = outlier_columns[1:]\n",
    "        \n",
    "# Replace the outlier with the median value in each of these columns\n",
    "for column in outlier_columns:\n",
    "    median_value = prev_app.loc[prev_app[column] != 365243, column].median()\n",
    "    prev_app.loc[prev_app[column] == 365243, column] = median_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## previous_application, get dummies and consolidate\n",
    "The data must be consolidated to so the values of SK_ID_CURR are unique. To do this I will us .groupby and an aggregate function. For the object data, I will aggregate by sum. For the numeric data, I will aggregate by mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make objects df with the sum returned\n",
    "prev_app_objects_columns = list(prev_app.loc[:,prev_app.dtypes == 'object'].columns)\n",
    "prev_app_objects_columns.append('SK_ID_CURR')\n",
    "prev_app_objects = prev_app.loc[:,prev_app_objects_columns]\n",
    "prev_app_objects_dummies = pd.get_dummies(prev_app_objects)\n",
    "prev_app_objects_sum = prev_app_objects_dummies.groupby(['SK_ID_CURR']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make numbers df with the mean returned\n",
    "prev_app_numbers_columns = list(prev_app.loc[:,prev_app.dtypes != 'object'].columns)\n",
    "prev_app_numbers = prev_app.loc[:,prev_app_numbers_columns]\n",
    "prev_app_numbers_mean = prev_app_numbers.groupby(['SK_ID_CURR']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(338857, 162)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recombine the object and numeric dataframes\n",
    "prev_app = prev_app_numbers_mean.join(prev_app_objects_sum)\n",
    "prev_app.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cash_balance, get dummies and consolidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dummies and group by SK_ID_CURR\n",
    "cash_balance_dummies = pd.get_dummies(cash_balance)\n",
    "cash_balance = cash_balance_dummies.groupby(['SK_ID_CURR']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## payment, consolidation\n",
    "I will create three features from this data. \n",
    "- The difference in days between installment and payment -> groupby SK_ID_CURR and summed\n",
    "- The difference in ammount between installment and payment -> groupby SK_ID_CURR and summed\n",
    "- The number of unique installment versions when grouped by SK_ID_CURR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make DAYS_DELTA column\n",
    "payments['DAYS_DELTA'] = payments.DAYS_ENTRY_PAYMENT - payments.DAYS_INSTALMENT\n",
    "\n",
    "# make AMT_DELTA column\n",
    "payments['AMT_DELTA'] = payments.AMT_INSTALMENT - payments.AMT_PAYMENT\n",
    "\n",
    "# make a df for the sum of DAYS_DELTA and AMT_DELTA\n",
    "payment_delta = payments[['SK_ID_CURR','DAYS_DELTA','AMT_DELTA']]\n",
    "\n",
    "# make new df of the SUM\n",
    "payment_delta_sum = payment_delta.groupby(['SK_ID_CURR']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the number of unique installment versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a separate df to count the number of unique payment versions\n",
    "payment_version = payments[['SK_ID_CURR','NUM_INSTALMENT_VERSION']]\n",
    "\n",
    "# make new df of the unique counts\n",
    "payment_version_unique = payment_version.groupby(['SK_ID_CURR']).nunique()\n",
    "\n",
    "# rename the NUM_INSTALMENT_VERSION columsn\n",
    "payment_version_unique.rename({\"NUM_INSTALMENT_VERSION\":\"NUM_INSTALMENT_UNIQUE\"},\n",
    "                              axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three variables are then combine to a new payment dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join payment_version_unique and payment_delta data frame\n",
    "payment = payment_version_unique.join(payment_delta_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cc_balance, consolidated\n",
    "With the cc_balance data set, I will create a new feature for holding an balance over the credit limit. This will be called 'AMT_OVER_LIMIT', and cases not over the limit will be set to 0.<p>\n",
    "Then I will group the object features by SK_ID_CURR and sum, and similarly group the numeric features by SK_ID_CURR and aggregate by the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering make column for balance over the credit limit\n",
    "cc_balance['AMT_OVER_LIMIT'] = cc_balance.AMT_BALANCE - cc_balance.AMT_CREDIT_LIMIT_ACTUAL\n",
    "\n",
    "# Set any negative values to 0\n",
    "cc_balance.loc[cc_balance.AMT_OVER_LIMIT <0, 'AMT_OVER_LIMIT'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make objects df with the dummy variables summed\n",
    "cc_balance_objects = cc_balance.loc[:,['SK_ID_CURR','NAME_CONTRACT_STATUS']]\n",
    "cc_balance_objects_dummies = pd.get_dummies(cc_balance_objects)\n",
    "cc_balance_objects_sum = cc_balance_objects_dummies.groupby(['SK_ID_CURR']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make numbers df with the mean returned \n",
    "cc_balance_numbers = cc_balance.drop(['NAME_CONTRACT_STATUS'], axis='columns')\n",
    "cc_balance_numbers_dummies = pd.get_dummies(cc_balance_numbers)\n",
    "cc_balance_numbers_mean = cc_balance_numbers_dummies.groupby(['SK_ID_CURR']).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join cc_balance_objects_sum and cc_balance_numbers_sum\n",
    "cc_balance = cc_balance_numbers_mean.join(cc_balance_objects_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (307500, 247)\n",
      "test (48744, 247)\n",
      "bureau (305811, 43)\n",
      "prev_app (338857, 162)\n",
      "cash_balance (337252, 14)\n",
      "payments (13605401, 9)\n",
      "cc_balance (103558, 28)\n"
     ]
    }
   ],
   "source": [
    "print('train',train.shape)\n",
    "print('test',test.shape)\n",
    "print('bureau',bureau.shape)\n",
    "print('prev_app',prev_app.shape)\n",
    "print('cash_balance',cash_balance.shape)\n",
    "print('payments',payments.shape)\n",
    "print('cc_balance',cc_balance.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename columns that will collide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau.rename({'AMT_ANNUITY': 'AMT_ANNUITY_bureau'}, axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in prev_app.columns:\n",
    "    new_column_name = column + '_prev_app'\n",
    "    prev_app.rename({column: new_column_name}, axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in cash_balance.columns:\n",
    "    new_column_name = column + '_cash_balance'\n",
    "    cash_balance.rename({column: new_column_name}, axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Join data sets\n",
    "Expected number of columns after joining all sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "503"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape[1] + bureau.shape[1] + prev_app.shape[1] + cc_balance.shape[1] + payments.shape[1] + cash_balance.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Form final training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307500, 503)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all = train.join(bureau).join(prev_app).join(cc_balance).join(payments).join(cash_balance)\n",
    "train_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Form final test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48744, 503)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_all = test.join(bureau).join(prev_app).join(cc_balance).join(payments).join(cash_balance)\n",
    "test_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export data as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all.to_csv('train_all.csv')\n",
    "test_all.to_csv('test_all.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
